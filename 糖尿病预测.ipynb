{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-01T08:10:18.636797Z",
     "iopub.status.busy": "2025-12-01T08:10:18.636484Z",
     "iopub.status.idle": "2025-12-01T08:10:18.649938Z",
     "shell.execute_reply": "2025-12-01T08:10:18.648289Z",
     "shell.execute_reply.started": "2025-12-01T08:10:18.636774Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T08:10:18.652634Z",
     "iopub.status.busy": "2025-12-01T08:10:18.652075Z",
     "iopub.status.idle": "2025-12-01T08:10:25.908877Z",
     "shell.execute_reply": "2025-12-01T08:10:25.906902Z",
     "shell.execute_reply.started": "2025-12-01T08:10:18.652533Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, gc\n",
    "\n",
    "train = pd.read_csv(r'C:\\Users\\11150\\Desktop\\data_analyse\\Diabetes_Prediction_Challenge\\dataset\\train.csv')\n",
    "test = pd.read_csv(r'C:\\Users\\11150\\Desktop\\data_analyse\\Diabetes_Prediction_Challenge\\dataset\\test.csv')\n",
    "orig = pd.read_csv(r'C:\\Users\\11150\\Desktop\\data_analyse\\Diabetes_Prediction_Challenge\\dataset\\diabetes_dataset.csv')\n",
    "print('Train Shape:', train.shape)\n",
    "print('Test Shape:', test.shape)\n",
    "print('Orig Shape:', orig.shape)\n",
    "\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T08:10:25.911435Z",
     "iopub.status.busy": "2025-12-01T08:10:25.911019Z",
     "iopub.status.idle": "2025-12-01T08:10:25.965889Z",
     "shell.execute_reply": "2025-12-01T08:10:25.964711Z",
     "shell.execute_reply.started": "2025-12-01T08:10:25.911401Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TARGET = 'diagnosed_diabetes'\n",
    "BASE = [col for col in train.columns if col not in ['id', TARGET]]\n",
    "CATS = train.select_dtypes('object').columns.to_list()\n",
    "NUMS = [col for col in BASE if col not in CATS]\n",
    "print(f'{len(BASE)} Base Features:{BASE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some EDA for Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check Unique Values & Missing Data\n",
    "First, let's check the number of unique values and missing values for both **NUMS** and **CATS** features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T08:10:25.969626Z",
     "iopub.status.busy": "2025-12-01T08:10:25.969339Z",
     "iopub.status.idle": "2025-12-01T08:10:26.74323Z",
     "shell.execute_reply": "2025-12-01T08:10:26.741865Z",
     "shell.execute_reply.started": "2025-12-01T08:10:25.969605Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print('NaN Count:', train[CATS].isnull().sum().sum(), '\\n')\n",
    "print(train[CATS].nunique(),'\\n')\n",
    "train[CATS].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T08:10:26.745264Z",
     "iopub.status.busy": "2025-12-01T08:10:26.744855Z",
     "iopub.status.idle": "2025-12-01T08:10:27.010838Z",
     "shell.execute_reply": "2025-12-01T08:10:27.009835Z",
     "shell.execute_reply.started": "2025-12-01T08:10:26.745232Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print('NaN Count:', train[NUMS].isnull().sum().sum(), '\\n')\n",
    "print(train[NUMS].nunique(),'\\n')\n",
    "train[NUMS].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "* **No Missing Values:** The dataset is clean, containing zero null values.\n",
    "* **Low Cardinality:** Even the numerical features exhibit relatively low unique counts (cardinality) considering the large dataset size of roughly 700k samples.\n",
    "* **Feature Engineering Idea:** Given this discrete nature, **Target Encoding** could be effective in extracting more signals from Numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Target Distribution\n",
    "We examine the distribution of the target variable `diagnosed_diabetes`. This helps us determine if we need to address class imbalance techniques or use stratified cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T08:10:27.012419Z",
     "iopub.status.busy": "2025-12-01T08:10:27.01206Z",
     "iopub.status.idle": "2025-12-01T08:10:28.084558Z",
     "shell.execute_reply": "2025-12-01T08:10:28.083389Z",
     "shell.execute_reply.started": "2025-12-01T08:10:27.012387Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T08:10:28.086041Z",
     "iopub.status.busy": "2025-12-01T08:10:28.085577Z",
     "iopub.status.idle": "2025-12-01T08:10:28.779198Z",
     "shell.execute_reply": "2025-12-01T08:10:28.778087Z",
     "shell.execute_reply.started": "2025-12-01T08:10:28.086008Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Train Dataset\n",
    "sns.countplot(data=train, x=TARGET, ax=ax[0], palette='viridis')\n",
    "ax[0].set_title(f'Train: {TARGET} Distribution')\n",
    "for p in ax[0].patches:\n",
    "    ax[0].annotate(f'{p.get_height():,}', (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "# Original Dataset (if target exists)\n",
    "if TARGET in orig.columns:\n",
    "    sns.countplot(data=orig, x=TARGET, ax=ax[1], palette='viridis')\n",
    "    ax[1].set_title(f'Original: {TARGET} Distribution')\n",
    "    for p in ax[1].patches:\n",
    "        ax[1].annotate(f'{p.get_height():,}', (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                       ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Feature Distributions (Train vs Test vs Original)\n",
    "Next, we visualize the distributions of numerical features across the Train, Test, and Original datasets. It is crucial to confirm that the Test set follows a similar distribution to the Train set to ensure model generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T08:10:28.781659Z",
     "iopub.status.busy": "2025-12-01T08:10:28.781081Z",
     "iopub.status.idle": "2025-12-01T08:12:28.470399Z",
     "shell.execute_reply": "2025-12-01T08:12:28.469122Z",
     "shell.execute_reply.started": "2025-12-01T08:10:28.781605Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_plot = pd.concat([\n",
    "    train[NUMS].assign(Source='Train'),\n",
    "    test[NUMS].assign(Source='Test'),\n",
    "    orig[NUMS].assign(Source='Original')\n",
    "])\n",
    "\n",
    "n_cols = 3\n",
    "n_rows = (len(NUMS) + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(NUMS):\n",
    "    sns.kdeplot(data=df_plot, x=col, hue='Source', ax=axes[i], \n",
    "                fill=True, common_norm=False, warn_singular=False)\n",
    "    axes[i].set_title(col)\n",
    "\n",
    "for i in range(len(NUMS), len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "del df_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Correlation Matrix\n",
    "Finally, we analyze the correlation between numerical features and the target. This heatmap helps identify multicollinearity and highlights features that have a strong linear relationship with the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T08:12:28.473943Z",
     "iopub.status.busy": "2025-12-01T08:12:28.473608Z",
     "iopub.status.idle": "2025-12-01T08:12:30.962365Z",
     "shell.execute_reply": "2025-12-01T08:12:30.960056Z",
     "shell.execute_reply.started": "2025-12-01T08:12:28.473918Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "corr_features = NUMS + [TARGET]\n",
    "corr_matrix = train[corr_features].corr()\n",
    "\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=\".2f\", \n",
    "            cmap='coolwarm', vmin=-1, vmax=1, linewidths=0.5)\n",
    "plt.title('Correlation Matrix (Numerical Features vs Target)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations & Strategy\n",
    "Based on the EDA above, we can draw several key insights to guide our modeling strategy:\n",
    "\n",
    "1.  **Distribution Alignment:** The feature distributions of **Train** and **Test** are nearly identical, which is excellent for model generalization and validates our local CV strategy.\n",
    "2.  **Original Data Strategy:** The **Original** dataset shows distinct distributional differences (sharper peaks and shifts). Simply concatenating it for data augmentation might introduce noise (covariate shift). Instead, it may be more effective to use the Original data for **Feature Engineering**—such as creating aggregate features or robust target encoding—to extract underlying biological signals.\n",
    "3.  **Multicollinearity:** We observe a strong correlation (**0.81**) between `ldl_cholesterol` and `cholesterol_total`. While tree-based models like XGBoost can handle this, we should be aware of this redundancy when analyzing feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering\n",
    "We will implement several feature engineering techniques to boost model performance.\n",
    "\n",
    "## Original Data Features (External Source Encoding)\n",
    "As noted in the EDA, the **Original Dataset** distribution differs from Train/Test. Instead of simple concatenation (which risks covariate shift), we utilize the Original data as an external reference to create statistical features:\n",
    "\n",
    "* **Orig Mean (Target Encoding):** The probability of diabetes for a given category/value in the real-world dataset. This serves as a robust, leakage-free risk indicator.\n",
    "* **Orig Count (Frequency Encoding):** How frequently a value appears in the original medical records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T08:12:30.963773Z",
     "iopub.status.busy": "2025-12-01T08:12:30.963454Z",
     "iopub.status.idle": "2025-12-01T08:12:45.271376Z",
     "shell.execute_reply": "2025-12-01T08:12:45.270052Z",
     "shell.execute_reply.started": "2025-12-01T08:12:30.963744Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ORIG = []\n",
    "\n",
    "for col in BASE:\n",
    "    # MEAN\n",
    "    mean_map = orig.groupby(col)[TARGET].mean()\n",
    "    new_mean_col_name = f\"orig_mean_{col}\"\n",
    "    mean_map.name = new_mean_col_name\n",
    "    \n",
    "    train = train.merge(mean_map, on=col, how='left')\n",
    "    test = test.merge(mean_map, on=col, how='left')\n",
    "    ORIG.append(new_mean_col_name)\n",
    "\n",
    "    # COUNT\n",
    "    new_count_col_name = f\"orig_count_{col}\"\n",
    "    count_map = orig.groupby(col).size().reset_index(name=new_count_col_name)\n",
    "    \n",
    "    train = train.merge(count_map, on=col, how='left')\n",
    "    test = test.merge(count_map, on=col, how='left')\n",
    "    ORIG.append(new_count_col_name)\n",
    "\n",
    "print(f'{len(ORIG)} ORIG Features Created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T08:12:45.272844Z",
     "iopub.status.busy": "2025-12-01T08:12:45.272581Z",
     "iopub.status.idle": "2025-12-01T08:12:45.514592Z",
     "shell.execute_reply": "2025-12-01T08:12:45.513366Z",
     "shell.execute_reply.started": "2025-12-01T08:12:45.272824Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for col in ORIG:\n",
    "    if 'mean' in col:\n",
    "        train[col] = train[col].fillna(orig[TARGET].mean())\n",
    "        test[col] = test[col].fillna(orig[TARGET].mean())\n",
    "    else:\n",
    "        train[col] = train[col].fillna(0)\n",
    "        test[col] = test[col].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Training\n",
    "\n",
    "## 4.1 Feature Set Aggregation\n",
    "We consolidate all feature groups created during the engineering phase:\n",
    "* **BASE:** The original features from the train dataset.\n",
    "* **ORIG:** Statistical features derived from the external original dataset.\n",
    "\n",
    "We then define our feature matrix `X` and target vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T08:12:45.515776Z",
     "iopub.status.busy": "2025-12-01T08:12:45.515483Z",
     "iopub.status.idle": "2025-12-01T08:12:45.521209Z",
     "shell.execute_reply": "2025-12-01T08:12:45.520126Z",
     "shell.execute_reply.started": "2025-12-01T08:12:45.515754Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "FEATURES = BASE + ORIG\n",
    "print(len(FEATURES), 'Features.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T08:12:45.522703Z",
     "iopub.status.busy": "2025-12-01T08:12:45.522358Z",
     "iopub.status.idle": "2025-12-01T08:12:45.800599Z",
     "shell.execute_reply": "2025-12-01T08:12:45.799485Z",
     "shell.execute_reply.started": "2025-12-01T08:12:45.522673Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X = train[FEATURES]\n",
    "y = train[TARGET]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Robust Target Encoder with Internal CV\n",
    "We implement a custom `TargetEncoder` class designed to extract signals from categorical features while strictly preventing **Data Leakage**.\n",
    "\n",
    "Key features of this implementation:\n",
    "* **Internal K-Fold CV:** Encodes training data using out-of-fold statistics (`fit_transform`), ensuring the model doesn't see its own label during training.\n",
    "* **Smoothing:** Applies regularization (Empirical Bayes) to prevent overfitting on rare categories.\n",
    "* **Multiple Aggregations:** Supports not just `mean`, but also `std`, `count`, etc., to capture the full distribution of the target per category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T08:12:45.80187Z",
     "iopub.status.busy": "2025-12-01T08:12:45.80162Z",
     "iopub.status.idle": "2025-12-01T08:12:46.121605Z",
     "shell.execute_reply": "2025-12-01T08:12:46.120465Z",
     "shell.execute_reply.started": "2025-12-01T08:12:45.80185Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Target Encoder that supports multiple aggregation functions,\n",
    "    internal cross-validation for leakage prevention, and smoothing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cols_to_encode : list of str\n",
    "        List of column names to be target encoded.\n",
    "\n",
    "    aggs : list of str, default=['mean']\n",
    "        List of aggregation functions to apply. Any function accepted by\n",
    "        pandas' `.agg()` method is supported, such as:\n",
    "        'mean', 'std', 'var', 'min', 'max', 'skew', 'nunique', \n",
    "        'count', 'sum', 'median'.\n",
    "        Smoothing is applied only to the 'mean' aggregation.\n",
    "\n",
    "    cv : int, default=5\n",
    "        Number of folds for cross-validation in fit_transform.\n",
    "\n",
    "    smooth : float or 'auto', default='auto'\n",
    "        The smoothing parameter `m`. A larger value puts more weight on the \n",
    "        global mean. If 'auto', an empirical Bayes estimate is used.\n",
    "        \n",
    "    drop_original : bool, default=False\n",
    "        If True, the original columns to be encoded are dropped.\n",
    "    \"\"\"\n",
    "    def __init__(self, cols_to_encode, aggs=['mean'], cv=5, smooth='auto', drop_original=False):\n",
    "        self.cols_to_encode = cols_to_encode\n",
    "        self.aggs = aggs\n",
    "        self.cv = cv\n",
    "        self.smooth = smooth\n",
    "        self.drop_original = drop_original\n",
    "        self.mappings_ = {}\n",
    "        self.global_stats_ = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learn mappings from the entire dataset.\n",
    "        These mappings are used for the transform method on validation/test data.\n",
    "        \"\"\"\n",
    "        temp_df = X.copy()\n",
    "        temp_df['target'] = y\n",
    "\n",
    "        # Learn global statistics for each aggregation\n",
    "        for agg_func in self.aggs:\n",
    "            self.global_stats_[agg_func] = y.agg(agg_func)\n",
    "\n",
    "        # Learn category-specific mappings\n",
    "        for col in self.cols_to_encode:\n",
    "            self.mappings_[col] = {}\n",
    "            for agg_func in self.aggs:\n",
    "                mapping = temp_df.groupby(col)['target'].agg(agg_func)\n",
    "                self.mappings_[col][agg_func] = mapping\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply learned mappings to the data.\n",
    "        Unseen categories are filled with global statistics.\n",
    "        \"\"\"\n",
    "        X_transformed = X.copy()\n",
    "        for col in self.cols_to_encode:\n",
    "            for agg_func in self.aggs:\n",
    "                new_col_name = f'TE_{col}_{agg_func}'\n",
    "                map_series = self.mappings_[col][agg_func]\n",
    "                X_transformed[new_col_name] = X[col].map(map_series)\n",
    "                X_transformed[new_col_name].fillna(self.global_stats_[agg_func], inplace=True)\n",
    "        \n",
    "        if self.drop_original:\n",
    "            X_transformed.drop(columns=self.cols_to_encode, inplace=True)\n",
    "            \n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit and transform the data using internal cross-validation to prevent leakage.\n",
    "        \"\"\"\n",
    "        # First, fit on the entire dataset to get global mappings for transform method\n",
    "        self.fit(X, y)\n",
    "\n",
    "        # Initialize an empty DataFrame to store encoded features\n",
    "        encoded_features = pd.DataFrame(index=X.index)\n",
    "        \n",
    "        kf = KFold(n_splits=self.cv, shuffle=True, random_state=42)\n",
    "\n",
    "        for train_idx, val_idx in kf.split(X, y):\n",
    "            X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            \n",
    "            temp_df_train = X_train.copy()\n",
    "            temp_df_train['target'] = y_train\n",
    "\n",
    "            for col in self.cols_to_encode:\n",
    "                # --- Calculate mappings only on the training part of the fold ---\n",
    "                for agg_func in self.aggs:\n",
    "                    new_col_name = f'TE_{col}_{agg_func}'\n",
    "                    \n",
    "                    # Calculate global stat for this fold\n",
    "                    fold_global_stat = y_train.agg(agg_func)\n",
    "                    \n",
    "                    # Calculate category stats for this fold\n",
    "                    mapping = temp_df_train.groupby(col)['target'].agg(agg_func)\n",
    "\n",
    "                    # --- Apply smoothing only for 'mean' aggregation ---\n",
    "                    if agg_func == 'mean':\n",
    "                        counts = temp_df_train.groupby(col)['target'].count()\n",
    "                        \n",
    "                        m = self.smooth\n",
    "                        if self.smooth == 'auto':\n",
    "                            # Empirical Bayes smoothing\n",
    "                            variance_between = mapping.var()\n",
    "                            avg_variance_within = temp_df_train.groupby(col)['target'].var().mean()\n",
    "                            if variance_between > 0:\n",
    "                                m = avg_variance_within / variance_between\n",
    "                            else:\n",
    "                                m = 0  # No smoothing if no variance between groups\n",
    "                        \n",
    "                        # Apply smoothing formula\n",
    "                        smoothed_mapping = (counts * mapping + m * fold_global_stat) / (counts + m)\n",
    "                        encoded_values = X_val[col].map(smoothed_mapping)\n",
    "                    else:\n",
    "                        encoded_values = X_val[col].map(mapping)\n",
    "                    \n",
    "                    # Store encoded values for the validation fold\n",
    "                    encoded_features.loc[X_val.index, new_col_name] = encoded_values.fillna(fold_global_stat)\n",
    "\n",
    "        # Merge with original DataFrame\n",
    "        X_transformed = X.copy()\n",
    "        for col in encoded_features.columns:\n",
    "            X_transformed[col] = encoded_features[col]\n",
    "            \n",
    "        if self.drop_original:\n",
    "            X_transformed.drop(columns=self.cols_to_encode, inplace=True)\n",
    "            \n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Stratified K-Fold Training (XGBoost)\n",
    "We train an XGBoost model using the Scikit-Learn API within a 5-fold Stratified Cross-Validation loop. \n",
    "To maximize model performance and maintain stability, we apply specific preprocessing strategies inside the fold:\n",
    "\n",
    "1.  **Selective Target Encoding:** We apply Target Encoding to numerical features with **more than 2 unique values**. Binary features are excluded to prevent noise.\n",
    "2.  **Native Categorical Support:** For categorical columns (`CATS`), we apply factorization (Label Encoding) and cast them to the `category` data type. This enables XGBoost's `enable_categorical=True` to handle categorical splits optimally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-01T08:18:01.611Z",
     "iopub.execute_input": "2025-12-01T08:17:16.630097Z",
     "iopub.status.busy": "2025-12-01T08:17:16.629731Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import gc\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 0. Helper: Memory Reducer\n",
    "# -----------------------------------------------------\n",
    "def reduce_mem_usage(df):\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object and col_type.name != 'category':\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "    return df\n",
    "\n",
    "X = reduce_mem_usage(X)\n",
    "test = reduce_mem_usage(test)\n",
    "gc.collect()\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 4.3 Training Loop (XGBoost)\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# Initialize arrays\n",
    "oof_preds = np.zeros(len(X))\n",
    "test_preds = np.zeros(len(test))\n",
    "\n",
    "# Stratified K-Fold\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Select TE Columns (Exclude binary features)\n",
    "TE_COLS = [col for col in NUMS if train[col].nunique() > 2]\n",
    "print(f\"Target Encoding applied to {len(TE_COLS)} features.\")\n",
    "\n",
    "# XGBoost Parameters\n",
    "xgb_params = {\n",
    "    'n_estimators': 20000,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'eval_metric': 'auc',\n",
    "    'device': 'cuda',           # GPU (optional)\n",
    "    'enable_categorical': True  # Native Categorical Support\n",
    "}\n",
    "\n",
    "print(f\"Starting Training...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "    \n",
    "    # 1. Split Data\n",
    "    X_train, y_train = X.iloc[train_idx].copy(), y.iloc[train_idx]\n",
    "    X_val, y_val = X.iloc[val_idx].copy(), y.iloc[val_idx]\n",
    "    X_test_fold = test[FEATURES].copy() \n",
    "    \n",
    "    # -----------------------------------------------------\n",
    "    # A. Target Encoding (Augment Numerical Features)\n",
    "    # -----------------------------------------------------\n",
    "    if len(TE_COLS) > 0:\n",
    "        TE = TargetEncoder(cols_to_encode=TE_COLS, cv=5, smooth='auto', aggs=['mean', 'count'], drop_original=False)\n",
    "        X_train = TE.fit_transform(X_train, y_train)\n",
    "        X_val = TE.transform(X_val)\n",
    "        X_test_fold = TE.transform(X_test_fold)\n",
    "    \n",
    "    # -----------------------------------------------------\n",
    "    # B. Factorize CATS & Prepare for Native Support\n",
    "    # -----------------------------------------------------\n",
    "    for c in CATS:\n",
    "        # 1. Factorize (Returns NumPy Array)\n",
    "        combined = pd.concat([X_train[c], X_val[c], X_test_fold[c]])\n",
    "        combined_encoded, _ = combined.factorize()\n",
    "        \n",
    "        # 2. Assign back to DataFrame to convert to Series\n",
    "        X_train[c] = combined_encoded[:len(X_train)]\n",
    "        X_val[c] = combined_encoded[len(X_train):len(X_train)+len(X_val)]\n",
    "        X_test_fold[c] = combined_encoded[len(X_train)+len(X_val):]\n",
    "\n",
    "        # 3. Cast to Category\n",
    "        X_train[c] = X_train[c].astype('category')\n",
    "        X_val[c] = X_val[c].astype('category')\n",
    "        X_test_fold[c] = X_test_fold[c].astype('category')\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # C. Train Model (XGBClassifier)\n",
    "    # -----------------------------------------------------\n",
    "    model = XGBClassifier(**xgb_params)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        early_stopping_rounds=200,\n",
    "        verbose=500\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    val_preds = model.predict_proba(X_val)[:, 1]\n",
    "    oof_preds[val_idx] = val_preds\n",
    "    test_preds += model.predict_proba(X_test_fold)[:, 1] / kf.get_n_splits()\n",
    "    \n",
    "    fold_score = roc_auc_score(y_val, val_preds)\n",
    "    print(f\"Fold {fold+1} AUC: {fold_score:.5f}\")\n",
    "\n",
    "    if len(TE_COLS) > 0: del TE\n",
    "    gc.collect()\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"OOF AUC: {roc_auc_score(y, oof_preds):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Feature Importance Analysis\n",
    "We visualize the top features contributing to the model's predictions.\n",
    "This step is crucial to:\n",
    "1.  **Validate Feature Engineering:** Confirm if our new features (e.g., `TE_orig_mean...`, interactions) are providing strong signals.\n",
    "2.  **Model Interpretability:** Understand the driving factors behind diabetes diagnosis in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-01T08:13:56.559541Z",
     "iopub.status.idle": "2025-12-01T08:13:56.560211Z",
     "shell.execute_reply": "2025-12-01T08:13:56.559996Z",
     "shell.execute_reply.started": "2025-12-01T08:13:56.559954Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Extract importances from the last trained model\n",
    "# (Note: Ideally we average importances across all folds, but the last fold serves as a good proxy for a baseline)\n",
    "imp_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 12))\n",
    "sns.barplot(data=imp_df.head(40), x='Importance', y='Feature', palette='viridis')\n",
    "plt.title('Top 40 Feature Importances (XGBoost - Last Fold)')\n",
    "plt.xlabel('Importance (Gain)')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Submission & Verification\n",
    "We prepare the final submission file using the averaged predictions from the 5-fold cross-validation.\n",
    "Additionally, we save the **Out-Of-Fold (OOF)** predictions for future ensembling and visualize the prediction distribution to perform a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-01T08:13:56.563527Z",
     "iopub.status.idle": "2025-12-01T08:13:56.563872Z",
     "shell.execute_reply": "2025-12-01T08:13:56.563737Z",
     "shell.execute_reply.started": "2025-12-01T08:13:56.563724Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1. Create Submission DataFrame\n",
    "sub = pd.read_csv('/kaggle/input/playground-series-s5e12/sample_submission.csv')\n",
    "sub[TARGET] = test_preds\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "\n",
    "# 2. Save OOF Predictions (for Ensembling)\n",
    "# OOF dataframe creating ensures we match the correct IDs\n",
    "oof_df = pd.DataFrame()\n",
    "oof_df['id'] = train['id']\n",
    "oof_df[TARGET] = y\n",
    "oof_df['pred'] = oof_preds\n",
    "oof_df.to_csv('oof_predictions.csv', index=False)\n",
    "\n",
    "print('Submission and OOF files saved successfully.')\n",
    "print(f'Submission Shape: {sub.shape}')\n",
    "\n",
    "# 3. Sanity Check: Distribution Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.kdeplot(oof_df['pred'], label='OOF Predictions (Train)', fill=True, color='blue', alpha=0.3)\n",
    "sns.kdeplot(sub[TARGET], label='Test Predictions', fill=True, color='orange', alpha=0.3)\n",
    "plt.title('Distribution of Predictions: OOF vs Test')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14272474,
     "sourceId": 91723,
     "sourceType": "competition"
    },
    {
     "datasetId": 8316713,
     "sourceId": 13128284,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
